{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Notebook.ipynb.txt",
      "provenance": [],
      "collapsed_sections": [
        "F7xCGA6Z8zbL",
        "FkNodA1y8zbm",
        "qZlGBdHA8zcE",
        "z0zK2a--8zcK",
        "sLpQh8ky8zcP",
        "B8wmfNO78zcT",
        "3bdx3bft8zcd",
        "dVgVH5In8zcn",
        "ZhY6SD-I8zcr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KashyapSim/Sentiment-Analysis-CNN/blob/master/Notebook_ipynb_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4hr4Ay-B8zbD"
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from gensim import models\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7xCGA6Z8zbL"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oBztj0n8zbM"
      },
      "source": [
        "data = pd.read_csv('/imdb_labelled.tsv.txt', header = None, delimiter='\\t')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5I4sreW8zbQ"
      },
      "source": [
        "data.columns = ['Text', 'Label']"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOz5smkp8zbT",
        "outputId": "f7927b2f-0e8b-40ab-b544-f593f6bac3b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Label\n",
              "0  A very, very, very slow-moving, aimless movie ...      0\n",
              "1  Not sure who was more lost - the flat characte...      0\n",
              "2  Attempting artiness with black & white and cle...      0\n",
              "3       Very little music or anything to speak of.        0\n",
              "4  The best scene in the movie was when Gerardo i...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO12u6e08zbW",
        "outputId": "1b68ce02-cfcb-4e88-dace-455274f99382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data.Label.unique()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YvHGYnI8zba",
        "outputId": "f9cd5160-d25b-4449-b71d-ba6253945c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(748, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBJnW2aG8zbf"
      },
      "source": [
        "pos = []\n",
        "neg = []\n",
        "for l in data.Label:\n",
        "    if l == 0:\n",
        "        pos.append(0)\n",
        "        neg.append(1)\n",
        "    elif l == 1:\n",
        "        pos.append(1)\n",
        "        neg.append(0)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OwovrwU8zbi"
      },
      "source": [
        "data['Pos']= pos\n",
        "data['Neg']= neg"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbETUYGv8zbk",
        "outputId": "b3fab945-9452-46d3-a3c9-2579575cec91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Label  Pos  Neg\n",
              "0  A very, very, very slow-moving, aimless movie ...      0    0    1\n",
              "1  Not sure who was more lost - the flat characte...      0    0    1\n",
              "2  Attempting artiness with black & white and cle...      0    0    1\n",
              "3       Very little music or anything to speak of.        0    0    1\n",
              "4  The best scene in the movie was when Gerardo i...      1    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkNodA1y8zbm"
      },
      "source": [
        "### Clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQbDXw58zbn"
      },
      "source": [
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct\n",
        "\n",
        "data['Text_Clean'] = data['Text'].apply(lambda x: remove_punct(x))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsSk4phy8zbq",
        "outputId": "60ae11b4-1df6-4c14-c894-5f57c2859783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('punkt')\n",
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "tokens = [word_tokenize(sen) for sen in data.Text_Clean] "
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myXAz1LB8zbt"
      },
      "source": [
        "def lower_token(tokens): \n",
        "    return [w.lower() for w in tokens]    \n",
        "    \n",
        "lower_tokens = [lower_token(token) for token in tokens] "
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q9Cq7fu8zbz",
        "outputId": "64062a10-8a03-4511-bbb0-26503f7352aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFUpqnjY8zb1"
      },
      "source": [
        "def remove_stop_words(tokens): \n",
        "    return [word for word in tokens if word not in stoplist]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK1wtElZ8zb3"
      },
      "source": [
        "filtered_words = [remove_stop_words(sen) for sen in lower_tokens] "
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIymxOcB8zb5"
      },
      "source": [
        "result = [' '.join(sen) for sen in filtered_words] "
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-9WFWzQ8zb7"
      },
      "source": [
        "data['Text_Final'] = result"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlZWBxVL8zb9"
      },
      "source": [
        "data['tokens'] = filtered_words"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8BwLbWG8zb_"
      },
      "source": [
        "data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYioJ66a8zcB",
        "outputId": "7c5cd383-88eb-4da0-eda1-4327731705bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data[:4]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text_Final</th>\n",
              "      <th>tokens</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>slowmoving aimless movie distressed drifting y...</td>\n",
              "      <td>[slowmoving, aimless, movie, distressed, drift...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sure lost flat characters audience nearly half...</td>\n",
              "      <td>[sure, lost, flat, characters, audience, nearl...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>attempting artiness black white clever camera ...</td>\n",
              "      <td>[attempting, artiness, black, white, clever, c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>little music anything speak</td>\n",
              "      <td>[little, music, anything, speak]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Text_Final  ... Neg\n",
              "0  slowmoving aimless movie distressed drifting y...  ...   1\n",
              "1  sure lost flat characters audience nearly half...  ...   1\n",
              "2  attempting artiness black white clever camera ...  ...   1\n",
              "3                        little music anything speak  ...   1\n",
              "\n",
              "[4 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZlGBdHA8zcE"
      },
      "source": [
        "### Split data into test and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmQIiqbE8zcE"
      },
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60z84e3l8zcG",
        "outputId": "cedebdc3-e5bf-40bb-f0b3-497c2591f981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7218 words total, with a vocabulary size of 2881\n",
            "Max sentence length is 789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y8EoRUa8zcI",
        "outputId": "1ba57e25-cf76-42a8-af66-a350eb53fc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580 words total, with a vocabulary size of 457\n",
            "Max sentence length is 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0zK2a--8zcK"
      },
      "source": [
        "### Load Google News Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50CFzUjc8zcL",
        "outputId": "72ce078b-1214-4738-977e-3af04f75f843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "\n",
        "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkearvTQ8zcN"
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLpQh8ky8zcP"
      },
      "source": [
        "### Get Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kni71DX48zcQ"
      },
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N65QZUTJ8zcR"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8wmfNO78zcT"
      },
      "source": [
        "### Tokenize and Pad sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgBts6WB8zcU",
        "outputId": "30fda871-5867-453c-c58a-212e082b5223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2881 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8S-mP6e8zcV"
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlgGV7b8zcX",
        "outputId": "dd743ccf-8d6c-4fbd-8897-6110dce6a867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2882, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbsgg2Il8zca"
      },
      "source": [
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
        "\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdx3bft8zcd"
      },
      "source": [
        "### Define CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36LE3SRJ8zcd"
      },
      "source": [
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjESifyD8zcf"
      },
      "source": [
        "label_names = ['Pos', 'Neg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zi1o8M7G8zcg"
      },
      "source": [
        "y_train = data_train[label_names].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FYa-5MU8zci"
      },
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KzLEZH38zcl",
        "outputId": "36f714c7-1334-48fe-bf85-837d3f7cd979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 300)      864600      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 49, 200)      120200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 48, 200)      180200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 47, 200)      240200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 46, 200)      300200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 45, 200)      360200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 200)          0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 200)          0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 200)          0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_3 (GlobalM (None, 200)          0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_4 (GlobalM (None, 200)          0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 1000)         0           global_max_pooling1d[0][0]       \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_max_pooling1d_3[0][0]     \n",
            "                                                                 global_max_pooling1d_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1000)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 128)          128128      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            258         dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,193,986\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 864,600\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVgVH5In8zcn"
      },
      "source": [
        "### Train CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61D-L64i8zco"
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 34"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "030_Kiz18zcp",
        "outputId": "b5b05fb0-cd53-405f-ce57-da937dffdb49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "18/18 [==============================] - 3s 179ms/step - loss: 0.6785 - acc: 0.6281 - val_loss: 0.6364 - val_acc: 0.7647\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - 3s 166ms/step - loss: 0.4602 - acc: 0.8661 - val_loss: 0.4556 - val_acc: 0.7941\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - 3s 164ms/step - loss: 0.2390 - acc: 0.9306 - val_loss: 0.4055 - val_acc: 0.7941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhY6SD-I8zcr"
      },
      "source": [
        "### Test CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmsp_EDK8zcr",
        "outputId": "28ae7f84-171b-4d1e-a25b-90e9a5b828da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3_SceLX8zct"
      },
      "source": [
        "labels = [1, 0]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76cEweiN8zcw"
      },
      "source": [
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])\n",
        "    "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy32s09_8zcy",
        "outputId": "b337e76b-7e04-4c43-fcd0-d55c8776d8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaQzC34u8zc0",
        "outputId": "282462c6-5087-40ff-ff49-b7f904f2fee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "data_test.Label.value_counts()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    44\n",
              "1    31\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPpgAIRiwUKM"
      },
      "source": [
        "testing test cases\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcib419dwVv0",
        "outputId": "7bede5c8-3eb9-4b63-c8b7-605ab95766ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "text=\"very awwful movie.\"\n",
        "#suspense feeling frustration retarded girls\"\n",
        "text= remove_punct(text)\n",
        "tokens = word_tokenize(text)\n",
        "tokens=lower_token(tokens)\n",
        "remove_stop_words(tokens)\n",
        "filtered_words = remove_stop_words(tokens) \n",
        "result = ' '.join(filtered_words)\n",
        "\n",
        "print(result)\n",
        "\n",
        "ts = tokenizer.texts_to_sequences([result])\n",
        "#print(np.shape(ts))\n",
        "#print(ts)\n",
        "\n",
        "tc = pad_sequences(ts, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "prediction = model.predict(tc)\n",
        "\n",
        "print(labels[np.argmax(prediction)])\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "awwful movie\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}